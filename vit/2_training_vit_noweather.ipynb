{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qdYdZgdLR1_s"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, time, timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import xarray as xr\n",
        "from ocf_blosc2 import Blosc2\n",
        "from torch.utils.data import DataLoader, IterableDataset\n",
        "from torchinfo import summary\n",
        "import json\n",
        "plt.rcParams[\"figure.figsize\"] = (20, 12)\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Wf080q8HR1_s"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezk-JK1bR1_u"
      },
      "source": [
        "## Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sh9uXxwNR1_v"
      },
      "outputs": [],
      "source": [
        "pv = pd.read_parquet(\"data/pv/2020/1.parquet\").drop(\"generation_wh\", axis=1)\n",
        "for i in range(2, 13):\n",
        "    pv2 = pd.read_parquet(f\"data/pv/2020/{i}.parquet\").drop(\"generation_wh\", axis=1)\n",
        "    pv = pd.concat([pv, pv2], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WAG7l2vXR1_v"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 1024\n",
        "hrv = xr.open_mfdataset(\"data/satellite-hrv/2020/*.zarr.zip\", engine=\"zarr\", chunks={\"time\": BATCH_SIZE * 2}, parallel=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrINou71R1_v"
      },
      "source": [
        "As part of the challenge, you can make use of satellite imagery, numerical weather prediction and air quality forecast data in a `[128, 128]` region centred on each solar PV site. In order to help you out, we have pre-computed the indices corresponding to each solar PV site and included them in `indices.json`, which we can load directly. For more information, take a look at the [challenge page](https://doxaai.com/competition/climatehackai-2023).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KbmT-SeAR1_v"
      },
      "outputs": [],
      "source": [
        "with open(\"indices.json\") as f:\n",
        "    site_locations = {\n",
        "        data_source: {\n",
        "            int(site): (int(location[0]), int(location[1]))\n",
        "            for site, location in locations.items()\n",
        "        }\n",
        "        for data_source, locations in json.load(f).items()\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkjhkOL7R1_w"
      },
      "source": [
        "### Defining a PyTorch Dataset\n",
        "\n",
        "To get started, we will define a simple `IterableDataset` that shows how to slice into the PV and HRV data using `pandas` and `xarray`, respectively. You will have to modify this if you wish to incorporate non-HRV data, weather forecasts and air quality forecasts into your training regimen. If you have any questions, feel free to ask on the [ClimateHack.AI Community Discord server](https://discord.gg/HTTQ8AFjJp)!\n",
        "\n",
        "**Note**: `site_locations` contains indices for the non-HRV, weather forecast and air quality forecast data as well as for the HRV data!\n",
        "\n",
        "There are many more advanced strategies you could implement to load data in training, particularly if you want to pre-prepare training batches in advance or use multiple workers to improve data loading times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ejpb1ZKiR1_w"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "month_to_times = {\n",
        "    1: (time(8), time(16)),\n",
        "    2: (time(8), time(17)),\n",
        "    3: (time(7), time(18)),\n",
        "    4: (time(7), time(19)),\n",
        "    5: (time(6), time(20)),\n",
        "    6: (time(5), time(20)),\n",
        "    7: (time(5), time(20)),\n",
        "    8: (time(6), time(20)),\n",
        "    9: (time(7), time(19)),\n",
        "    10: (time(7), time(18)),\n",
        "    11: (time(7), time(16)),\n",
        "    12: (time(8), time(16))\n",
        "}\n",
        "class ChallengeDataset(IterableDataset):\n",
        "    def __init__(self, pv, hrv, site_locations, sites=None, transform=None, min_date=None, max_date=None):\n",
        "        self.pv = pv\n",
        "        self.hrv = hrv\n",
        "        self._site_locations = site_locations\n",
        "        self.transform=transform\n",
        "        self._sites = sites if sites else list(site_locations[\"hrv\"].keys())\n",
        "        assert (min_date and max_date), \"Did not provide a min and/or max date range.\"\n",
        "        self.min_date = min_date\n",
        "        self.max_date = max_date\n",
        "    def __len__(self):\n",
        "        i = 0\n",
        "        for _ in self._get_image_times():\n",
        "            i += 1\n",
        "        i *= len(self._sites)\n",
        "        return i\n",
        "    def _get_image_times(self):\n",
        "        min_date = self.min_date\n",
        "        max_date = self.max_date\n",
        "        date = min_date\n",
        "        while date <= max_date:\n",
        "            start_time, end_time = month_to_times[date.month]\n",
        "            current_time = datetime.combine(date, start_time)\n",
        "            while current_time.time() < end_time:\n",
        "                if current_time:\n",
        "                    yield current_time\n",
        "\n",
        "                current_time += timedelta(minutes=60)\n",
        "\n",
        "            date += timedelta(days=1)\n",
        "\n",
        "    def __iter__(self):\n",
        "        for time in self._get_image_times():\n",
        "            first_hour = slice(str(time), str(time + timedelta(minutes=55)))\n",
        "            day_of_year = ((time - datetime(time.year, 1, 1)).days + 1)/365\n",
        "            pv_features = pv.xs(first_hour, drop_level=False)  # type: ignore\n",
        "            pv_targets = pv.xs(\n",
        "                slice(  # type: ignore\n",
        "                    str(time + timedelta(hours=1)),\n",
        "                    str(time + timedelta(hours=4, minutes=55)),\n",
        "                ),\n",
        "                drop_level=False,\n",
        "            )\n",
        "\n",
        "            hrv_data = self.hrv[\"data\"].sel(time=first_hour).to_numpy()\n",
        "\n",
        "            if np.isnan(hrv_data).any():\n",
        "                continue\n",
        "\n",
        "            for site in self._sites:\n",
        "                try:\n",
        "                    # Get solar PV features and targets\n",
        "                    site_features = pv_features.xs(site, level=1).to_numpy().squeeze(-1)\n",
        "                    site_targets = pv_targets.xs(site, level=1).to_numpy().squeeze(-1)\n",
        "                    assert site_features.shape == (12,) and site_targets.shape == (48,)\n",
        "\n",
        "                    # Get a 128x128 HRV crop centred on the site over the previous hour\n",
        "                    x, y = self._site_locations[\"hrv\"][site]\n",
        "                    hrv_features = hrv_data[:, y - 64 : y + 64, x - 64 : x + 64, 0]\n",
        "                    assert hrv_features.shape == (12, 128, 128)\n",
        "\n",
        "                    # How might you adapt this for the non-HRV, weather and aerosol data?\n",
        "                except:\n",
        "                    continue\n",
        "                if self.transform:\n",
        "                    hrv_features = self.transform(hrv_features)\n",
        "                yield day_of_year, site_features, hrv_features, site_targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8EGlRldR1_x"
      },
      "source": [
        "## Train a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gr5D2pdnR1_x"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train dataset len: 4182516\n"
          ]
        }
      ],
      "source": [
        "train_dataset = ChallengeDataset(pv, hrv, site_locations=site_locations, min_date=datetime(2020, 1, 1), max_date=datetime(2020, 12, 31))\n",
        "dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory=True)\n",
        "print(f\"train dataset len: {len(train_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CIPuwDufR1_y"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'device' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msubmission\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OurTransformer\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m OurTransformer(image_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[43mdevice\u001b[49m)\n\u001b[1;32m      3\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mL1Loss()\n\u001b[1;32m      4\u001b[0m optimiser \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
          ]
        }
      ],
      "source": [
        "from submission.model import OurTransformer, OurResnet\n",
        "model = OurResnet(image_size=128).to(device)\n",
        "criterion = nn.L1Loss()\n",
        "optimiser = optim.Adam(model.parameters(), lr=0.1)\n",
        "summary(model, input_size=[(1, 12), (1, 12, 128, 128), (1,)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4tuuVQz0R1_y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model key ViT-B32-2106.10270-Full-NoWeather\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/16338 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1, 100: 0.08203044341877103:   1%|          | 119/16338 [00:53<2:00:38,  2.24it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m optimiser\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39mdevice):\n\u001b[1;32m     13\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m     14\u001b[0m         pv_features\u001b[38;5;241m.\u001b[39mto(device,dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat),\n\u001b[0;32m---> 15\u001b[0m         \u001b[43mhrv_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     16\u001b[0m         dr\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m     17\u001b[0m     )\n\u001b[1;32m     18\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(predictions, pv_targets\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat))\n\u001b[1;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "EPOCHS = 100\n",
        "MODEL_KEY=\"ResNext50-Full-NoWeather\"\n",
        "print(f\"Training model key {MODEL_KEY}\")\n",
        "from tqdm import tqdm\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    count = 0\n",
        "    for i, (dr, pv_features, hrv_features, pv_targets) in (pbar := tqdm(enumerate(dataloader), total=len(dataloader), ascii=True)):\n",
        "        optimiser.zero_grad()\n",
        "        with torch.autocast(device_type=device):\n",
        "            predictions = model(\n",
        "                pv_features.to(device,dtype=torch.float),\n",
        "                hrv_features.to(device,dtype=torch.float),\n",
        "                dr.to(device, dtype=torch.float)\n",
        "            )\n",
        "            loss = criterion(predictions, pv_targets.to(device, dtype=torch.float))\n",
        "        loss.backward()\n",
        "\n",
        "        optimiser.step()\n",
        "\n",
        "        size = int(pv_targets.size(0))\n",
        "        running_loss += float(loss) * size\n",
        "        count += size\n",
        "\n",
        "        if i % 50 == 49:\n",
        "            pbar.set_description(f\"Epoch {epoch + 1}, {i + 1}: {running_loss / count}\")\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}: {running_loss / count}\")\n",
        "    torch.save(model.state_dict(), f\"submission/{MODEL_KEY}-ep{epoch + 1}.pt\")\n",
        "    print(\"Saved model!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbKDEnQZR1_y"
      },
      "outputs": [],
      "source": [
        "# Save your model\n",
        "torch.save(model.state_dict(), \"submission/model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
