{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdYdZgdLR1_s"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, time, timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import xarray as xr\n",
        "from ocf_blosc2 import Blosc2\n",
        "from torch.utils.data import DataLoader, IterableDataset\n",
        "from torchinfo import summary\n",
        "import json\n",
        "plt.rcParams[\"figure.figsize\"] = (20, 12)\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wf080q8HR1_s"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gr5D2pdnR1_x"
      },
      "outputs": [],
      "source": [
        "from dataset import HDF5Dataset\n",
        "dataset = HDF5Dataset(\"./data/ds14_processed_data/processed_train.hdf5\", True, True, True, True)\n",
        "data_loader = DataLoader(dataset, batch_size=16, pin_memory=True, num_workers=8, shuffle=True)\n",
        "print(f\"train dataset len: {len(dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIPuwDufR1_y"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 200\n",
        "START_EPOCH = 0\n",
        "from submission.model import OurResnet2\n",
        "model = OurResnet2(image_size=128, device=device).to(device)\n",
        "criterion = nn.L1Loss()\n",
        "optimiser = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.02)\n",
        "lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimiser, T_max=30, eta_min=3e-4)\n",
        "summary(model, input_size=[(1, 12), (1, 1, 12, 128, 128), (1, 10, 6, 128, 128), (1, 3)])\n",
        "# x = torch.randn((1, 12)).to(device)\n",
        "# y = torch.randn((1, 1, 12, 128, 128)).to(device)\n",
        "# z = torch.randn((1, 10, 6, 128, 128)).to(device)\n",
        "# a = torch.randn((1, 3)).to(device)\n",
        "# model(x, y, z, a)\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tuuVQz0R1_y"
      },
      "outputs": [],
      "source": [
        "MODEL_KEY=\"ExtraEmbedding_TemporalResnet2+1Combo-DeepFC\"\n",
        "print(f\"Training model key {MODEL_KEY}\")\n",
        "from tqdm import tqdm\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    count = 0\n",
        "    for i, (pv_features, hrv_features, nwp, extra, pv_targets) in (pbar := tqdm(enumerate(data_loader), total=len(data_loader), ascii=True)):\n",
        "        optimiser.zero_grad()\n",
        "        with torch.autocast(device_type=device):\n",
        "            real_extra = extra[:, 2:]\n",
        "            hrv_features = torch.unsqueeze(hrv_features, 1)\n",
        "            predictions = model(\n",
        "                pv_features.to(device,dtype=torch.float),\n",
        "                hrv_features.to(device,dtype=torch.float),\n",
        "                nwp.to(device,dtype=torch.float),\n",
        "                real_extra.to(device,dtype=torch.float),\n",
        "            )\n",
        "            loss = criterion(predictions, pv_targets.to(device, dtype=torch.float))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "        optimiser.step()\n",
        "\n",
        "        size = int(pv_targets.size(0))\n",
        "        running_loss += float(loss) * size\n",
        "        count += size\n",
        "\n",
        "        if i % 10 == 9:\n",
        "            writer.add_scalar(f\"Loss/ep{START_EPOCH + epoch + 1}\", (running_loss / count), i)\n",
        "            pbar.set_description(f\"Epoch {START_EPOCH + epoch + 1}, {i + 1}: {running_loss / count}\")\n",
        "        if i % 100 == 99:\n",
        "            print(f\"Epoch {START_EPOCH + epoch + 1}, {i + 1}: {running_loss / count}\")\n",
        "\n",
        "    lr_scheduler.step()\n",
        "    current_lr = lr_scheduler.get_last_lr()[0]\n",
        "    print(f\"Epoch {START_EPOCH + epoch + 1}: {running_loss / count} (LR: {current_lr})\")\n",
        "    writer.add_scalar(f\"Loss/train\", (running_loss / count), START_EPOCH + epoch + 1)\n",
        "    writer.add_scalar(f\"LR\", current_lr, START_EPOCH + epoch + 1)\n",
        "    torch.save(model.state_dict(), f\"/data/{MODEL_KEY}-ep{START_EPOCH + epoch + 1}.pt\")\n",
        "    print(\"Saved model!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
