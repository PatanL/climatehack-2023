{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdYdZgdLR1_s"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, time, timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import xarray as xr\n",
        "from ocf_blosc2 import Blosc2\n",
        "from torch.utils.data import DataLoader, IterableDataset\n",
        "from torchinfo import summary\n",
        "import json\n",
        "plt.rcParams[\"figure.figsize\"] = (20, 12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wf080q8HR1_s"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezk-JK1bR1_u"
      },
      "source": [
        "## Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sh9uXxwNR1_v"
      },
      "outputs": [],
      "source": [
        "pv = pd.read_parquet(\"data/pv/2020/1.parquet\").drop(\"generation_wh\", axis=1)\n",
        "for i in range(2, 13):\n",
        "    pv2 = pd.read_parquet(f\"data/pv/2020/{i}.parquet\").drop(\"generation_wh\", axis=1)\n",
        "    pv = pd.concat([pv, pv2], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAG7l2vXR1_v"
      },
      "outputs": [],
      "source": [
        "hrv = xr.open_dataset(\n",
        "    \"data/satellite-hrv/2020/1.zarr.zip\", engine=\"zarr\", chunks=\"auto\"\n",
        ")\n",
        "for i in range(2, 13):\n",
        "    hrv2 = xr.open_dataset(\n",
        "        f\"data/satellite-hrv/2020/{i}.zarr.zip\", engine=\"zarr\", chunks=\"auto\"\n",
        "    )\n",
        "    hrv = xr.concat((hrv, hrv2), dim=\"time\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrINou71R1_v"
      },
      "source": [
        "As part of the challenge, you can make use of satellite imagery, numerical weather prediction and air quality forecast data in a `[128, 128]` region centred on each solar PV site. In order to help you out, we have pre-computed the indices corresponding to each solar PV site and included them in `indices.json`, which we can load directly. For more information, take a look at the [challenge page](https://doxaai.com/competition/climatehackai-2023).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbmT-SeAR1_v"
      },
      "outputs": [],
      "source": [
        "with open(\"indices.json\") as f:\n",
        "    site_locations = {\n",
        "        data_source: {\n",
        "            int(site): (int(location[0]), int(location[1]))\n",
        "            for site, location in locations.items()\n",
        "        }\n",
        "        for data_source, locations in json.load(f).items()\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkjhkOL7R1_w"
      },
      "source": [
        "### Defining a PyTorch Dataset\n",
        "\n",
        "To get started, we will define a simple `IterableDataset` that shows how to slice into the PV and HRV data using `pandas` and `xarray`, respectively. You will have to modify this if you wish to incorporate non-HRV data, weather forecasts and air quality forecasts into your training regimen. If you have any questions, feel free to ask on the [ClimateHack.AI Community Discord server](https://discord.gg/HTTQ8AFjJp)!\n",
        "\n",
        "**Note**: `site_locations` contains indices for the non-HRV, weather forecast and air quality forecast data as well as for the HRV data!\n",
        "\n",
        "There are many more advanced strategies you could implement to load data in training, particularly if you want to pre-prepare training batches in advance or use multiple workers to improve data loading times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ejpb1ZKiR1_w"
      },
      "outputs": [],
      "source": [
        "class ChallengeDataset(IterableDataset):\n",
        "    def __init__(self, pv, hrv, site_locations, sites=None, transform=None, min_date=None, max_date=None):\n",
        "        self.pv = pv\n",
        "        self.hrv = hrv\n",
        "        self._site_locations = site_locations\n",
        "        self.transform=transform\n",
        "        self._sites = sites if sites else list(site_locations[\"hrv\"].keys())\n",
        "        assert (min_date and max_date), \"Did not provide a min and/or max date range.\"\n",
        "        self.min_date = min_date\n",
        "        self.max_date = max_date\n",
        "    def __len__(self):\n",
        "        i = 0\n",
        "        for _ in self._get_image_times():\n",
        "            i += 1\n",
        "        i *= len(self._sites)\n",
        "        return i\n",
        "    def _get_image_times(self):\n",
        "        min_date = self.min_date\n",
        "        max_date = self.max_date\n",
        "\n",
        "        start_time = time(8)\n",
        "        end_time = time(17)\n",
        "        date = min_date\n",
        "        while date <= max_date:\n",
        "            current_time = datetime.combine(date, start_time)\n",
        "            while current_time.time() < end_time:\n",
        "                if current_time:\n",
        "                    yield current_time\n",
        "\n",
        "                current_time += timedelta(minutes=60)\n",
        "\n",
        "            date += timedelta(days=1)\n",
        "\n",
        "    def __iter__(self):\n",
        "        for time in self._get_image_times():\n",
        "            first_hour = slice(str(time), str(time + timedelta(minutes=55)))\n",
        "\n",
        "            pv_features = pv.xs(first_hour, drop_level=False)  # type: ignore\n",
        "            pv_targets = pv.xs(\n",
        "                slice(  # type: ignore\n",
        "                    str(time + timedelta(hours=1)),\n",
        "                    str(time + timedelta(hours=4, minutes=55)),\n",
        "                ),\n",
        "                drop_level=False,\n",
        "            )\n",
        "\n",
        "            hrv_data = self.hrv[\"data\"].sel(time=first_hour).to_numpy()\n",
        "\n",
        "            for site in self._sites:\n",
        "                try:\n",
        "                    # Get solar PV features and targets\n",
        "                    site_features = pv_features.xs(site, level=1).to_numpy().squeeze(-1)\n",
        "                    site_targets = pv_targets.xs(site, level=1).to_numpy().squeeze(-1)\n",
        "                    assert site_features.shape == (12,) and site_targets.shape == (48,)\n",
        "\n",
        "                    # Get a 128x128 HRV crop centred on the site over the previous hour\n",
        "                    x, y = self._site_locations[\"hrv\"][site]\n",
        "                    hrv_features = hrv_data[:, y - 64 : y + 64, x - 64 : x + 64, 0]\n",
        "                    assert hrv_features.shape == (12, 128, 128)\n",
        "\n",
        "                    # How might you adapt this for the non-HRV, weather and aerosol data?\n",
        "                except:\n",
        "                    continue\n",
        "                if self.transform:\n",
        "                    hrv_features = self.transform(hrv_features)\n",
        "                yield site_features, hrv_features, site_targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8EGlRldR1_x"
      },
      "source": [
        "## Train a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gr5D2pdnR1_x"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "train_dataset = ChallengeDataset(pv, hrv, site_locations=site_locations, min_date=datetime(2020, 1, 1), max_date=datetime(2020, 12, 31))\n",
        "dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory=True)\n",
        "print(f\"train dataset len: {len(train_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIPuwDufR1_y"
      },
      "outputs": [],
      "source": [
        "from submission.model import OurTransformer\n",
        "model = OurTransformer(image_size=128).to(device)\n",
        "criterion = nn.L1Loss()\n",
        "optimiser = optim.Adam(model.parameters(), lr=1e-3)\n",
        "summary(model, input_size=[(1, 12), (1, 12, 128, 128)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tuuVQz0R1_y"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "MODEL_KEY=\"ViT-Tiny-Full\"\n",
        "from tqdm import tqdm\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    count = 0\n",
        "    for i, (pv_features, hrv_features, pv_targets) in (pbar := tqdm(enumerate(dataloader), total=len(dataloader))):\n",
        "        optimiser.zero_grad()\n",
        "\n",
        "        predictions = model(\n",
        "            pv_features.to(device, dtype=torch.float),\n",
        "            hrv_features.to(device, dtype=torch.float),\n",
        "        )\n",
        "        loss = criterion(predictions, pv_targets.to(device, dtype=torch.float))\n",
        "        loss.backward()\n",
        "\n",
        "        optimiser.step()\n",
        "\n",
        "        size = int(pv_targets.size(0))\n",
        "        running_loss += float(loss) * size\n",
        "        count += size\n",
        "\n",
        "        if i % 100 == 99:\n",
        "            pbar.set_description(f\"Epoch {epoch + 1}, {i + 1}: {running_loss / count}\")\n",
        "        if i == int(len(dataloader) * 0.5):\n",
        "            print(\"Saving halfway-point model...\")\n",
        "            torch.save(model.state_dict(), f\"submission/{MODEL_KEY}-ep{epoch + 1}-half.pt\")\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}: {running_loss / count}\")\n",
        "    torch.save(model.state_dict(), f\"submission/{MODEL_KEY}-ep{epoch + 1}.pt\")\n",
        "    print(\"Saved model!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbKDEnQZR1_y"
      },
      "outputs": [],
      "source": [
        "# Save your model\n",
        "torch.save(model.state_dict(), \"submission/model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
